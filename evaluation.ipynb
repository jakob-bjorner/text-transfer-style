{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport pandas as pd\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer\nfrom transformers import AdamW\nfrom transformers import get_scheduler\nfrom tqdm.auto import tqdm\nfrom datasets import load_metric\nfrom sklearn.model_selection import train_test_split\n\nbase_path = '/kaggle/input/nlp-getting-started/'\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-07T07:35:46.311456Z","iopub.execute_input":"2021-12-07T07:35:46.311794Z","iopub.status.idle":"2021-12-07T07:35:54.708625Z","shell.execute_reply.started":"2021-12-07T07:35:46.311761Z","shell.execute_reply":"2021-12-07T07:35:54.707898Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def convert_to_text(data):\n    text = data['text'].values\n    tokenizer_text = []\n    for t in text:\n        tokenizer_text.append(str(t))\n    return tokenizer_text\n\ndef convert_to_target(data):\n    return data['target'].values\n\ndef get_text_and_targets(data):\n    return convert_to_text(data), convert_to_target(data)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T07:35:59.334162Z","iopub.execute_input":"2021-12-07T07:35:59.335202Z","iopub.status.idle":"2021-12-07T07:35:59.342332Z","shell.execute_reply.started":"2021-12-07T07:35:59.335156Z","shell.execute_reply":"2021-12-07T07:35:59.340783Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class TransformerDataset(Dataset):\n    def __init__(self, text, target, tokenizer):\n        self.target = target\n        self.dict = tokenizer(text, padding=True, truncation=True)\n    \n    def __len__(self):\n        return len(self.dict['input_ids'])\n    \n    def __getitem__(self, ids):\n        if (self.target is None):\n            return {\n            'input_ids' : torch.tensor(self.dict['input_ids'][ids], dtype=torch.long),\n            'token_type_ids' : torch.tensor(self.dict['token_type_ids'][ids], dtype=torch.long),\n            'attention_mask' : torch.tensor(self.dict['attention_mask'][ids], dtype=torch.long),\n        }\n        else :\n            return {\n            'input_ids' : torch.tensor(self.dict['input_ids'][ids], dtype=torch.long),\n            'token_type_ids' : torch.tensor(self.dict['token_type_ids'][ids], dtype=torch.long),\n            'attention_mask' : torch.tensor(self.dict['attention_mask'][ids], dtype=torch.long),\n            'labels' : torch.tensor(self.target[ids], dtype=torch.long)\n        }","metadata":{"execution":{"iopub.status.busy":"2021-12-07T07:36:01.513561Z","iopub.execute_input":"2021-12-07T07:36:01.514165Z","iopub.status.idle":"2021-12-07T07:36:01.524591Z","shell.execute_reply.started":"2021-12-07T07:36:01.514100Z","shell.execute_reply":"2021-12-07T07:36:01.523576Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(base_path + 'train.csv')\ntest_data = pd.read_csv(base_path + 'test.csv')\n\n\ntrain_text_full, train_targets_full = get_text_and_targets(train_data)\n\ntrain_text, dev_text, train_target, dev_target = train_test_split(\n    train_text_full,\n    train_targets_full,\n    random_state=42,   # seed?\n    test_size=0.2,\n    stratify=train_targets_full)\n\ntest_text = convert_to_text(test_data)\n\ntokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n\ntrain_dataset = TransformerDataset(train_text, train_target, tokenizer)\ndev_dataset   = TransformerDataset(dev_text, dev_target, tokenizer)\ntest_dataset  = TransformerDataset(test_text, None, tokenizer)\n\n# change batch size here\nbatch_size = 8\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ndev_dataloader = DataLoader(dev_dataset, batch_size=batch_size)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T07:36:12.723468Z","iopub.execute_input":"2021-12-07T07:36:12.723926Z","iopub.status.idle":"2021-12-07T07:36:18.036249Z","shell.execute_reply.started":"2021-12-07T07:36:12.723895Z","shell.execute_reply":"2021-12-07T07:36:18.035342Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, dataloader):\n    metric = load_metric(\"accuracy\")\n    model.eval()\n    for batch in tqdm(dataloader):\n        batch = {k : v.to(device) for k, v in batch.items()}\n        with torch.no_grad():\n            outputs = model(**batch)\n        logits = outputs.logits\n        predictions = torch.argmax(logits, dim=-1)\n        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n    \n    return metric.compute()\n\ndef train(model, train_dataloader, initial_lr, nums_epochs, dev_dataloader):\n    n_training_steps = num_epochs * len(train_dataloader)\n    optimizer = AdamW(model.parameters(), lr=initial_lr)\n    lr_scheduler = get_scheduler(\"linear\",\n                                 optimizer=optimizer,\n                                 num_warmup_steps=0,\n                                 num_training_steps=n_training_steps)\n\n\n    for epoch in range(num_epochs):\n        model.train()\n        \n        epoch_loss = 0\n        num_batches = 0\n        \n        for batch in tqdm(train_dataloader):\n            batch = {k : v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            loss = outputs.loss\n            epoch_loss += loss.float()\n            num_batches += 1\n            \n            loss.backward()\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n        \n        print(f\"[Training]: Epoch=[{epoch}], loss=[{loss}]\")\n        \n        dev_accuracy = evaluate(model, dev_dataloader)\n        print(f\"[Dev]: accuracy=[{dev_accuracy}]\")\n        \ndef test(model, test_dataloader):\n    preds = torch.Tensor()\n    model.eval()\n    for batch in tqdm(test_dataloader):\n        batch = {k : v.to(device) for k, v in batch.items()}\n        with torch.no_grad():\n            outputs = model(**batch)\n        logits = outputs.logits\n        predictions = torch.argmax(logits, dim=-1)\n        preds = torch.cat((preds, predictions.cpu()))\n    \n    result = preds.numpy()\n    return result","metadata":{"execution":{"iopub.status.busy":"2021-12-07T07:38:34.170406Z","iopub.execute_input":"2021-12-07T07:38:34.170695Z","iopub.status.idle":"2021-12-07T07:38:34.195017Z","shell.execute_reply.started":"2021-12-07T07:38:34.170665Z","shell.execute_reply":"2021-12-07T07:38:34.193921Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T07:38:47.103762Z","iopub.execute_input":"2021-12-07T07:38:47.104423Z","iopub.status.idle":"2021-12-07T07:38:51.567788Z","shell.execute_reply.started":"2021-12-07T07:38:47.104335Z","shell.execute_reply":"2021-12-07T07:38:51.567121Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"num_epochs = 1\n\ntrain(model, train_dataloader, 5e-5, num_epochs, dev_dataloader)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nres = test(model, test_dataloader)\nprint(f\"[Test Results]: {res}\")\nsub = pd.read_csv(base_path + 'sample_submission.csv')\nsub['target'] = res\nsub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T07:51:30.094349Z","iopub.execute_input":"2021-12-07T07:51:30.094953Z","iopub.status.idle":"2021-12-07T07:51:33.268910Z","shell.execute_reply.started":"2021-12-07T07:51:30.094899Z","shell.execute_reply":"2021-12-07T07:51:33.267349Z"},"trusted":true},"execution_count":18,"outputs":[]}]}